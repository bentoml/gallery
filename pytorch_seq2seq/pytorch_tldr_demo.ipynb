{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a682ea0b",
   "metadata": {},
   "source": [
    "# BentoML PyTorch TLDR Tutorial\n",
    "\n",
    "Link to source code: https://github.com/bentoml/gallery/tree/main/TLDR/pytorch_tldr_demo/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ce1c9",
   "metadata": {},
   "source": [
    "Install required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad00863",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb49fd0",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45393b74",
   "metadata": {},
   "source": [
    "## Download and Extract Reddit TL;DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e227583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import requests\n",
    "from os.path import exists\n",
    "import zipfile\n",
    "import json\n",
    "from tqdm import trange\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "070965aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset/corpus-webis-tldr-17.zip'\n",
    "\n",
    "if not exists(dataset_path):\n",
    "    r = requests.get('https://zenodo.org/record/1043504/files/corpus-webis-tldr-17.zip?download=1')\n",
    "    with open(dataset_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    with zipfile.ZipFile(dataset_path, 'r') as z:\n",
    "        z.extractall('dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ed520",
   "metadata": {},
   "source": [
    "## Begin processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbbd3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 26\n",
    "\n",
    "#initialize Lang Class\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        #initialize containers to hold the words and corresponding index\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    #split a sentence into words and add it to the container\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    #If the word is not in the container, the word will be added to it, \n",
    "    #else, update the word counter\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e854567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    \"\"\"\n",
    "    A generator that iterates through a dataset file and\n",
    "    yields a tuple of (body, summary)\n",
    "    \"\"\"\n",
    "    def __init__(self, path='dataset/corpus-webis-tldr-17.json'):\n",
    "        self.df = pd.read_json(path, lines=True, chunksize=100000)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for d in self.df:\n",
    "            body = d['content']\n",
    "            summary = d['summary']\n",
    "            yield self._normalize_sentence(body), \\\n",
    "                self._normalize_sentence(summary)\n",
    "\n",
    "    #Normalize every sentence\n",
    "    def _normalize_sentence(self, sentence):\n",
    "        sentence = sentence.str.lower()\n",
    "        sentence = sentence.str.replace('[^A-Za-z\\s]+', '', regex=True)\n",
    "        sentence = sentence.str.normalize('NFD')\n",
    "        sentence = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "        return sentence\n",
    "\n",
    "    def process_data(self):\n",
    "        bodies, summaries = next(self.__iter__())\n",
    "\n",
    "        source = Lang()\n",
    "        target = Lang()\n",
    "        pairs = []\n",
    "        for body, summary in zip(bodies, summaries):\n",
    "            full = [body, summary]\n",
    "            source.addSentence(body)\n",
    "            target.addSentence(summary)\n",
    "            pairs.append(full)\n",
    "        return source, target, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8209c",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "First let's define a simple PyTorch network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46fd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(input_lang, output_lang, pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05e42d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        #set the encoder input dimesion , embbed dimesion, hidden dimesion, and number of layers \n",
    "        self.input_dim = input_dim\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #initialize the embedding layer with input and embbed dimention\n",
    "        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
    "        #intialize the GRU to take the input dimetion of embbed, and output dimention of hidden and\n",
    "        #set the number of gru layers\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src).view(1,1,-1)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        #set the encoder output dimension, embed dimension, hidden dimension, and number of layers \n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # initialize every layer with the appropriate dimension. For the decoder layer, it will consist of an embedding, GRU, a Linear layer and a Log softmax activation function.\n",
    "        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # reshape the input to (1, batch_size)\n",
    "        input = input.view(1, -1)\n",
    "        embedded = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded, hidden)       \n",
    "        prediction = self.softmax(self.out(output[0]))\n",
    "\n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        #initialize the encoder and decoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        input_length = source.size(0) #get the input length (number of words in sentence)\n",
    "        batch_size = target.shape[1] \n",
    "        target_length = target.shape[0]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "\n",
    "        #initialize a variable to hold the predicted outputs\n",
    "        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
    "\n",
    "        #encode every word in a sentence\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.encoder(source[i])\n",
    "\n",
    "            #use the encoder’s hidden layer as the decoder hidden\n",
    "            decoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "            #add a token before the first predicted word\n",
    "            decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
    "\n",
    "            #topk is used to get the top K value over a list\n",
    "            #predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value. \n",
    "\n",
    "            for t in range(target_length):   \n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                outputs[t] = decoder_output\n",
    "                teacher_force = random.random() < teacher_forcing_ratio\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                input = (target[t] if teacher_force else topi)\n",
    "                if(teacher_force == False and input.item() == EOS_token):\n",
    "                    break\n",
    "\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38888f0a",
   "metadata": {},
   "source": [
    "## Training and Saving the model\n",
    "\n",
    "Then we define a simple PyTorch network and some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bcda1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def clacModel(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
    "    model_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    loss = 0\n",
    "    epoch_loss = 0\n",
    "    # print(input_tensor.shape)\n",
    "\n",
    "    output = model(input_tensor, target_tensor)\n",
    "\n",
    "    num_iter = output.size(0)\n",
    "\n",
    "#calculate the loss from a predicted sentence with the expected result\n",
    "    for ot in range(num_iter):\n",
    "        loss += criterion(output[ot], target_tensor[ot])\n",
    "\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    epoch_loss = loss.item() / num_iter\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "def trainModel(model, source, target, pairs, num_iteration=20000):\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss_iterations = 0\n",
    "\n",
    "    training_pairs = [tensorsFromPair(source, target, random.choice(pairs))\n",
    "                     for i in range(num_iteration)]\n",
    "\n",
    "    for it in trange(1, num_iteration+1):\n",
    "        training_pair = training_pairs[it - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = clacModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
    "\n",
    "        total_loss_iterations += loss\n",
    "\n",
    "        if it % 5000 == 0:\n",
    "            avarage_loss = total_loss_iterations / 5000\n",
    "            total_loss_iterations = 0\n",
    "            losses.append(average_loss)\n",
    "            source, target, pairs = data.process_data()\n",
    "\n",
    "    torch.save(model.state_dict(), 'mytraining.pt')\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2f334de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, input_lang, output_lang, sentences):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
    "        output_tensor = tensorFromSentence(output_lang, sentences[1])\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        output = model(input_tensor, output_tensor)\n",
    "        # print(output_tensor)\n",
    "\n",
    "        for ot in range(output.size(0)):\n",
    "            topv, topi = output[ot].topk(1)\n",
    "            # print(topi)\n",
    "\n",
    "            if topi[0].item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi[0].item()])\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateRandomly(model, source, target, pairs, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('source {}'.format(pair[0]))\n",
    "        print('target {}'.format(pair[1]))\n",
    "        output_words = evaluate(model, source, target, pair)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('predicted {}'.format(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1d10bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sentence ['oh ive had so many fails  the worst of them all would have to be when i was in high school  i was excited to go to dinner and a movie with my totally not dating this dude but we hang out together all the time and cuddle on the bed while watching star trek friend  by dinner i mean mcdonalds  we were classy \\n i waited and waited and got hungry and waited some more and waited more  didnt hear anything from him and nobody answered the phone at his house  got  really  hungry  turned the oven on for a pizza  except id turned on the stove not the oven and id thrown my math notebook on top of the stove \\n i ran into the kitchen to see the notebook smoldering  i threw it on the floor and dumped a couple pitchers of water on it  singed the linoleum  turned the oven on put in the pizza  burned the pizza  the center was way overdone and i had to chisel it off  ate cereal instead \\n about six hours after i was supposed to get picked up his dad called me  turns out hed broken his leg on the ski slope earlier that day and was in the hospital  i scrambled around and managed to get a family friend to take me up to see him \\n the nurses at first wouldnt let me in to see him  so i channelled my ability to cry on command and said that but im his  girlfriend  i  need  to etc etc blubber blubber sniffle whine chin wobble etc etc  they changed their attitude really quick cooed at me said he needs your strength not your tears and helped me to his room where he was sleeping  i snickered and waited a couple hours for him to wake up  id brought a book to keep me company \\n so thats my vday disaster', 'thought i got stood up set my homework on fire burned my pizza and my notboyfriend broke his leg  o']\n",
      "Input : 236257 Output : 70371\n",
      "Encoder(\n",
      "  (embedding): Embedding(236257, 256)\n",
      "  (gru): GRU(256, 512)\n",
      ")\n",
      "Decoder(\n",
      "  (embedding): Embedding(70371, 256)\n",
      "  (gru): GRU(256, 512)\n",
      "  (out): Linear(in_features=512, out_features=70371, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.39s/it]\n"
     ]
    }
   ],
   "source": [
    "data = Data()\n",
    "source, target, pairs = data.process_data()\n",
    "\n",
    "randomize = random.choice(pairs)\n",
    "print('random sentence {}'.format(randomize))\n",
    "\n",
    "#print number of words\n",
    "input_size = source.n_words\n",
    "output_size = target.n_words\n",
    "print('Input : {} Output : {}'.format(input_size, output_size))\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_iteration = 2\n",
    "\n",
    "#create encoder-decoder model\n",
    "encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
    "decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "#print model \n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "model, losses = trainModel(model, source, target, pairs, num_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a87c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(model, source, target, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c19a0",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "We can do some cross validation and the results can be saved with the model as metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9b23c",
   "metadata": {},
   "source": [
    "### saving the model with some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = test_model(trained_model, test_loader)\n",
    "metadata = {\n",
    "    \"accuracy\": float(correct)/total,\n",
    "    \"cv_stats\": cv_results,\n",
    "}\n",
    "\n",
    "tag = bentoml.pytorch.save(\n",
    "    \"pytorch_tldr\",\n",
    "    trained_model,\n",
    "    metadata=metadata,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf35e55",
   "metadata": {},
   "source": [
    "## Create a BentoML Service for serving the model\n",
    "\n",
    "Note: using `%%writefile` here because `bentoml.Service` instance must be created in a separate `.py` file\n",
    "\n",
    "Even though we have only one model, we can create as many api endpoints as we want. Here we create two end points `predict_ndarray` and `predict_image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile service.py\n",
    "\n",
    "import typing as t\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from PIL.Image import Image as PILImage\n",
    "\n",
    "import bentoml\n",
    "from bentoml.io import Image\n",
    "from bentoml.io import NumpyNdarray\n",
    "\n",
    "\n",
    "tldr_runner = bentoml.pytorch.load_runner(\n",
    "    \"pytorch_tldr\",\n",
    "    name=\"tldr_runner\",\n",
    "    predict_fn_name=\"predict\",\n",
    ")\n",
    "\n",
    "svc = bentoml.Service(\n",
    "    name=\"pytorch_tldr_demo\",\n",
    "    runners=[\n",
    "        tldr_runner,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590147aa",
   "metadata": {},
   "source": [
    "Start a dev model server to test out the service defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29173871",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml serve service.py:svc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c1b36",
   "metadata": {},
   "source": [
    "Now you can use something like:\n",
    "\n",
    "`curl -H \"Content-Type: multipart/form-data\" -F'fileobj=@samples/1.png;type=image/png' http://127.0.0.1:5000/predict_image`\n",
    "    \n",
    "to send an image to the digit recognition service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f03564",
   "metadata": {},
   "source": [
    "## Build a Bento for distribution and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207561bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bentoml.build(\n",
    "    \"service.py:svc\",\n",
    "    include=[\"*.py\"],\n",
    "    exclude=[\"tests/\"],\n",
    "    description=\"file:./README.md\",\n",
    "    python=dict(\n",
    "        packages=[\"scikit-learn\", \"torch\", \"Pillow\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36306933",
   "metadata": {},
   "source": [
    "Starting a dev server with the Bento build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b9dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml serve pytorch_tldr_demo:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fae93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "name": "pytorch_mnist.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
